apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: mdp-gold-finalize
  namespace: md-pipeline
spec:
  mode: cluster
  serviceAccount: spark-runner
  sparkImage:
    productVersion: "3.5.6"
  mainApplicationFile: local:///opt/scripts/gold_finalize.py
  volumes:
    - name: scripts
      configMap:
        name: mdp-spark-scripts
  driver:
    cores: 2
    memory: 6Gi
    config:
      volumeMounts:
        - name: scripts
          mountPath: /opt/scripts
  dynamicAllocation:
    enabled: false
  executor:
    cores: 2
    memory: 6Gi
    config:
      volumeMounts:
        - name: scripts
          mountPath: /opt/scripts
  env:
    - name: PYTHONPATH
      value: "/opt/scripts"
    - name: MDP_ICEBERG_CATALOG
      value: "ice"
  s3connection:
    inline:
      host: 10.21.227.158
      port: 80
      accessStyle: Path
      credentials:
        secretClass: mdp-s3-credentials-class
  sparkConf:
    # Iceberg and Hive integration
    spark.jars.packages: "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1"
    spark.sql.extensions: "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
    spark.sql.catalog.ice: "org.apache.iceberg.spark.SparkCatalog"
    spark.sql.catalog.ice.type: "hive"
    spark.sql.catalog.ice.uri: "thrift://mdp-hive-metastore.md-pipeline.svc.cluster.local:9083"
    spark.sql.catalog.ice.warehouse: "s3a://mdp-gold/"
    spark.sql.catalog.ice.s3.endpoint: "http://10.21.227.158:80"
    spark.sql.catalog.ice.s3.path-style-access: "true"

    # S3A filesystem configuration
    spark.hadoop.fs.s3a.endpoint: "http://10.21.227.158:80"
    spark.hadoop.fs.s3a.path.style.access: "true"
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    
    # S3A performance optimizations (lighter for Gold workload)
    spark.hadoop.fs.s3a.fast.upload: "true"
    spark.hadoop.fs.s3a.fast.upload.buffer: "bytebuffer"
    spark.hadoop.fs.s3a.multipart.size: "134217728"
    spark.hadoop.fs.s3a.multipart.threshold: "134217728"
    spark.hadoop.fs.s3a.fast.upload.active.blocks: "8"
    spark.hadoop.fs.s3a.connection.maximum: "512"
    spark.hadoop.fs.s3a.threads.max: "256"
    spark.hadoop.fs.s3a.connection.request.timeout: "60000"
    spark.hadoop.fs.s3a.connection.timeout: "60000"
    spark.hadoop.fs.s3a.attempts.maximum: "5"
    spark.hadoop.fs.s3a.retry.limit: "5"

    # Iceberg-specific configurations
    spark.sql.iceberg.handle-timestamp-without-timezone: "true"
    spark.sql.sources.commitProtocolClass: "org.apache.iceberg.spark.source.SparkCommitProtocol"
    
    # Spark performance tuning for lightweight aggregation
    spark.sql.shuffle.partitions: "1024"
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
    spark.sql.adaptive.skewJoin.enabled: "true"
    
    # Memory and performance tuning
    spark.executor.memoryOverhead: "1536"
    spark.driver.memoryOverhead: "1024"
    spark.network.timeout: "1800s"
    spark.executor.heartbeatInterval: "60s"
    
    # Kubernetes executor management
    spark.dynamicAllocation.enabled: "false"
    spark.executor.instances: "24"
    spark.kubernetes.executor.deleteOnTermination: "false"
    spark.kubernetes.allocation.batch.size: "8"
    spark.kubernetes.allocation.podCreation.parallelism: "8"
    
    # JVM optimizations
    spark.executor.extraJavaOptions: "-XX:+UseG1GC -XX:MaxDirectMemorySize=1g -XX:MaxMetaspaceSize=512m"
    spark.driver.extraJavaOptions: "-XX:+UseG1GC -XX:MaxDirectMemorySize=2g"
    
    # Network and RPC configurations for stability
    spark.rpc.askTimeout: "1200s"
    spark.rpc.lookupTimeout: "120s"
    spark.kubernetes.driver.service.account: "spark-runner"
    spark.kubernetes.executor.service.account: "spark-runner"
    spark.kubernetes.authenticate.driver.serviceAccountName: "spark-runner"
    